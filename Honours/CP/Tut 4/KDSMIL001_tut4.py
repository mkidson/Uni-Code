# KDSMIL001
# 26-05-2022

import numpy as np
import random
import matplotlib.pyplot as plt
from scipy import randn

def metropolisMethod(N, probDist, xMin, xMax, x0, Delta=1):
    """Metropolis Method
    
        Args
        ----
            N (int):
        Number of random values to be generated
    
            probDist (function):
        The probability distribution that the numbers should be generated according to
    
            xMin (float):
        Lower bound of the interval on which to generate values
    
            xMax (float):
        Upper bound of the interval on which to generate values

            x0 (float):
        x-value to start the method at, should be the point at which `probDist` is at its max.

            Delta (int, optional): 
        Half the range over which to generate numbers when determining a trial point. Defaults to 1.

        Returns
        -------
            xChain (array, float):
        Array of a Markov chain containing all points generated by the Metropolis method

            acceptanceRatio (float):
        Ratio of values accepted, in terms of the Metropolis method, to total values generated
    """
    xChain = []
    xi = x0     # Sets current step to initial guess
    numAccepted = 0     # Will be used to keep track of acceptance ratio

    while len(xChain) < N:
        deltaI = random.uniform(-Delta, Delta)
        xTrial = xi + deltaI        # Generates new trial step randomly
        w = probDist(xTrial)/probDist(xi)
        xChain.append(xi)       # Appends current step to the chain. Done here since we always want to add to the chain, regardless of if we accept or not, so this cuts down on unnecessary logic later on

        if np.logical_or(xTrial < xMin, xTrial > xMax):     # Check if the trial is within the bounds and if not, starts again, importantly we have still added the current step to the chain
            continue
        
        # Pretty self-explanatory method for accepting or rejecting. Not sure if generating r here is a good idea, might be quicker to generate a whole list of them before we loop
        if w >= 1:
            xi = xTrial
            numAccepted += 1
            continue
        else:
            r = random.random()

            if r <= w:
                xi = xTrial
                numAccepted += 1
                continue
            else:
                continue

    acceptanceRatio = numAccepted / len(xChain)

    return xChain, acceptanceRatio

def autocorrelation(x, j):
    return (np.mean(x[:-j]*x[j:])-np.mean(x[:-j])**2)/(np.mean(x[:-j]**2)-np.mean(x[:-j])**2)

def movingAvg(xArr):
    """Returns a list of moving averages for some array `xArr`
    """

    xArrCumsum = np.cumsum(xArr)
    xArrMovingAvg = []

    for i in range(len(xArr)):
        xArrMovingAvg.append(xArrCumsum[i] / (i + 1))
    
    return np.array(xArrMovingAvg)

def correlationAndEquilibration(chain):
    """Finds the autocorrelation function for a Markov chain and returns the first `j` for which `C(j) <= 0.01`. Plots the autocorrelation function, as well as the moving average of the chain, then asks the user to input a number of steps after which it looks like equilibrium has been reached
    
        Args
        ----
            chain (array, float):
        Array of randomly generated numbers, hopefully a Markov chain produced from the Metropolis method
    
        Returns
        -------
            autoCorrSkip (int):
        Size of gap to skip each time a point is sampled from `chain`

            equilibrationSkip (int):
        Number of entries in `chain` to skip before starting to sample from it
    """
    # Computing the autocorrelation for a range of gap sizes
    autoCorrs = []
    for i in np.arange(1,50):
        autoCorr = autocorrelation(np.array(chain), i)
        autoCorrs.append(autoCorr)

    # Finds the first time the autocorrelation goes below 0.01, assigns the associated gap value, and breaks 
    for c in autoCorrs:
        if np.abs(c) < 1e-2:
            print(f'C(j={autoCorrs.index(c)}) = 0\n')
            autoCorrSkip = autoCorrs.index(c)
            break
    
    # Plotting autocorrelation function
    plt.figure()
    plt.plot(np.arange(1,50), autoCorrs, 'rs', ms=3)
    plt.axvline(autoCorrSkip, label=f'First C(j) < 0.01\nj = {autoCorrSkip}')
    plt.legend()
    plt.title('Plot of autocorrelation function')
    plt.xlabel('j')
    plt.ylabel('C(j)')

    finalAvg = np.mean(chain)       # Finds final average to compare the moving average to
    xMovingAverage = movingAvg(chain)   # Computes array of moving averages

    # Plotting the moving average for inspection
    plt.figure()
    plt.plot(xMovingAverage, 'rs', ms=3, label='Moving average')
    plt.axhline(finalAvg, label='Final average')
    plt.legend()
    plt.title('Moving average of the Markov Chain')
    plt.xlabel('Number of points generated')
    plt.ylabel('Average of x')

    plt.show()

    equilibriumPosition = int(input('Input equilibrium position: '))        # Takes input of an equilibrium position

    return autoCorrSkip, equilibriumPosition

def q1b():

    xMin = 0        # left BC x value
    xMax = 1        # Right BC x value
    u0 = 2          # Left BC u value
    uNp1 = 1        # Right BC u value
    deltaX = 0.001      # grid spacing
    eps = 0.0001       # Constant in diff eq
    numPoints = int((xMax - xMin) / deltaX)     # Number of points in the grid

    xs = np.linspace(xMin, xMax, numPoints)     # Array of x values 
    # Constants derived for this scheme
    alpha = np.full(numPoints, (eps + deltaX / 2))
    beta = np.full(numPoints, (eps - deltaX / 2))
    gamma = np.full(numPoints, (-2 * eps))

    # Tridiagonal matrix. Weird indexing going on here but it works
    A = np.diag(alpha[2:-1], -1) + np.diag(gamma[1:-1], 0) + np.diag(beta[1:-2], 1)
    w = np.zeros(numPoints - 2)     # RHS of the matrix eqn
    w[0] = -alpha[0] * u0       # Adding in the ends of w, from derivation
    w[-1] = -beta[-1] * uNp1

    u = np.linalg.solve(A, w)       # Solving the matrix eqn
    u = np.insert(u, 0, u0)         # Adding in the BCs
    u = np.append(u, uNp1)

    # Plotting the solution
    plt.plot(xs, u, label='Finite Difference solution')
    plt.xlabel('$x$')
    plt.ylabel('$u(x)$')
    plt.legend()
    plt.title('Solution to one-dimensional advection-diffusion equation, $\epsilon=0.1$')
    plt.grid(color='#CCCCCC', linestyle=':')
    plt.show()

def q1c():
    # Now with upwinding

    xMin = 0        # left BC x value
    xMax = 1        # Right BC x value
    u0 = 2          # Left BC u value
    uNp1 = 1        # Right BC u value
    deltaX = 0.001      # grid spacing
    eps = 0.0001       # Constant in diff eq
    numPoints = int((xMax - xMin) / deltaX)     # Number of points in the grid

    xs = np.linspace(xMin, xMax, numPoints)     # Array of x values 
    # Constants derived for this scheme
    alpha = np.full(numPoints, (eps + deltaX))
    beta = np.full(numPoints, (eps))
    gamma = np.full(numPoints, (-2 * eps - deltaX))

    # Tridiagonal matrix. Weird indexing going on here but it works
    A = np.diag(alpha[2:-1], -1) + np.diag(gamma[1:-1], 0) + np.diag(beta[1:-2], 1)
    w = np.zeros(numPoints - 2)     # RHS of the matrix eqn
    w[0] = -alpha[0] * u0       # Adding in the ends of w, from derivation
    w[-1] = -beta[-1] * uNp1

    u = np.linalg.solve(A, w)       # Solving the matrix eqn
    u = np.insert(u, 0, u0)         # Adding in the BCs
    u = np.append(u, uNp1)

    # Plotting the solution
    plt.plot(xs, u, label='Finite Difference solution')
    plt.xlabel('$x$')
    plt.ylabel('$u(x)$')
    plt.legend()
    plt.title('Solution to one-dimensional advection-diffusion equation, $\epsilon=0.0001$')
    plt.grid(color='#CCCCCC', linestyle=':')
    plt.show()

def q1d():
    xMin = 0        # left BC x value
    xMax = 1        # Right BC x value
    u0 = 2          # Left BC u value
    uNp1 = 1        # Right BC u value
    deltaX = 0.01      # grid spacing for x
    deltaT = 0.00001       # grid spacing for t, need to be chosen carefully to avoid instability
    eps = 0.1       # Constant in diff eq
    numPoints = int((xMax - xMin) / deltaX)     # Number of points in the grid

    xs = np.linspace(xMin, xMax, numPoints)     # Array of x values 
    # Constants derived for this scheme
    alpha = np.full(numPoints, (( eps / deltaX**2 ) + ( 1 / ( 2 * deltaX ) )) * deltaT)
    beta = np.full(numPoints, (( eps / deltaX**2 ) - ( 1 / ( 2 * deltaX ) )) * deltaT)
    gamma = np.full(numPoints, (( -2 * eps / deltaX**2 ) * deltaT ) + 1)

    IC = - xs + 2       # Choosing some initial condition that suits the BCs, this was simplest

    A = np.diag(alpha[2:-1], -1) + np.diag(gamma[1:-1], 0) + np.diag(beta[1:-2], 1)     # Matrix update equation, effectively

    dudts = []      # Array of du/dt, which will be a sum over the whole solution
    oldSoln = IC
    plt.plot(xs, oldSoln, label='Initial Condition')

    dudt = 1        # Setting it 1 here just to get things going

    numSteps = 0
    while dudt > 0.001:     # Fairly arbitrary condition but it seemed to give good results
        newSoln = np.matmul(A, oldSoln[1:-1])
        newSoln[0] += alpha[0] * u0     # Need to be added, comes from the derivation
        newSoln[-1] += beta[-1] * uNp1  # Same here
        newSoln = np.insert(newSoln, 0, u0)     # Adding in the ends, as in the method before
        newSoln = np.append(newSoln, uNp1)

        dudt = np.sum((newSoln - oldSoln) / deltaT)     # Measures difference in solution from one step to the next
        dudts.append(dudt)

        oldSoln = newSoln
        numSteps += 1

    plt.plot(xs, newSoln, label=f'Soln after {numSteps} steps')

    plt.xlabel('$x$')
    plt.ylabel('$u(x)$')
    plt.legend()
    plt.title('Solution to one-dimensional advection-diffusion equation, $\epsilon=0.1$')
    plt.grid(color='#CCCCCC', linestyle=':')
    plt.show()

def isingModel(T):
    # Outputs a magentisation for a given T
    # Could be made more general but I don't really feel like doing it if I'm honest

    J = 1       # Ferromagnetic exchange constant
    # kB = 1.380649e-34
    kB = 1

    # Create the random grid
    grid = np.zeros((10, 10))
    for row in grid:
        for i in range(len(row)):
            # Needed a little logic to decide whether to make it spin up or down
            randNum = random.random()
            if randNum < 0.5:
                row[i] = -1
            elif randNum >= 0.5:
                row[i] = 1

    # Now the loop
    for i in range(30000):
        ranRow, ranCol = np.random.randint(0, 10, 2)    # Getting random position in grid
        flipVal = grid[ranRow, ranCol] * -1     # Gets the value of that position, flipped

        # Calculates the energy change for the one position
        deltaE = 0
        # Using modulus here in order to never run out of bounds, since some of these indices will end up above 10. could be mod some variable but i don't wanna
        deltaE += -2 * J * flipVal * grid[ranRow%10, (ranCol-1)%10]
        deltaE += -2 * J * flipVal * grid[ranRow%10, (ranCol+1)%10]
        deltaE += -2 * J * flipVal * grid[(ranRow-1)%10, ranCol%10]
        deltaE += -2 * J * flipVal * grid[(ranRow+1)%10, ranCol%10]

        # Accepts the flip if the energy change is negative
        if deltaE <= 0:
            grid[ranRow, ranCol] = flipVal
        # If it's not negative, it accepts with probability p
        elif deltaE > 0:
            r = random.random()
            p = np.exp(-deltaE / (kB * T))
            if r <= p:
                grid[ranRow, ranCol] = flipVal
    
    magnetisation = np.mean(grid)       # Magnetisation as a ratio to fully magnetised, i.e. all pointing up or down.
    print(magnetisation)

    return magnetisation

def q2():
    mags = []
    ts = np.linspace(0,8,20)

    for i in ts:
        mags.append(np.abs(isingModel(i)))      # Using absolute value here so that we get something that is more easily interpreted
    
    plt.plot(ts, mags, 'o')
    plt.show()

if __name__ == "__main__":

    # q1b()

    # q1c()

    # q1d()

    # isingModel(1)

    q2()




    pass